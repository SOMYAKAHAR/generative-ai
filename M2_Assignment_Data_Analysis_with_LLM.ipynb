{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SOMYAKAHAR/generative-ai/blob/main/M2_Assignment_Data_Analysis_with_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiaipc6AKdWV"
      },
      "source": [
        "# Module 2 Coding Assignment: Retail Sales Data Analysis with LLM\n",
        "\n",
        "\n",
        "In this assignment, you will use LangChain and LLMs to perform advanced data analysis tasks that previously required extensive coding. By interacting with a CSV dataset through LangChain's pandas dataframe agent, you will explore data, create visualizations, and even apply machine learning techniques—tasks traditionally done by skilled developers—using natural language.\n",
        "\n",
        "# Why and How is it differnt from M1 assignment\n",
        "In the previous assignment, we demonstrated how you can interact with LLMs in a browser to perform advanced data analysis. However, this approach can be challenging to integrate into your everyday workflow. In this assignment, we’ll show you how to leverage a Langchain agent to handle the heavy lifting—running the code generated by the LLM, debugging it until you achieve the desired results, and seamlessly integrating the output into your existing data analysis/visualization pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1iuAekUcU5Z"
      },
      "source": [
        "# Senario: Columbia Plus Mart Transaction Data Analysis\n",
        "You are a Sales Analyst at Columbia Plus Mart, a retail store with a wide range of products across different categories. Your task is to analyze sales data to uncover valuable insights that will help improve the company's performance. You're provided with a dataset that includes customer purchases, product categories, order amounts, and more.\n",
        "\n",
        "# Tasks\n",
        "## 1. Data Exploration\n",
        "\n",
        "Use the Langchain agent with pandas to explore and clean the dataset. Identify any missing values, outliers, or trends.\n",
        "\n",
        "## 2. Sales Analysis\n",
        "\n",
        "Explore when order hikes or drops occur (e.g., specific months, days of the week, or during sales events).\n",
        "\n",
        "## 3. Customer Insights with Machine Learning\n",
        "\n",
        "Identify different customer groups and which product categories they tend to favor the most. This might involve clustering based on spending patterns, demographics, or product categories.\n",
        "\n",
        "## 4. Hypothesis Testing\n",
        "Based on everything we learn about our customers in Task 1, 2, and 3, we might have some hypothesis about our customers and we will be testing a few hypothesis to see if they are statistically sound.\n",
        "\n",
        "# More on the dataset\n",
        "You can download the dataset here: https://www.kaggle.com/datasets/manjeetsingh/retaildataset\n",
        "\n",
        "The dataset contains the following columns:\n",
        "\n",
        "* Transaction ID: Unique identifier for each transaction.\n",
        "\n",
        "* Date: The date of the transaction.\n",
        "\n",
        "* Customer ID: Unique identifier for each customer.\n",
        "\n",
        "* Gender: The gender of the customer.\n",
        "\n",
        "* Age: The age of the customer.\n",
        "\n",
        "* Product Category: The category of the product purchased (e.g., Beauty, Clothing, Electronics).\n",
        "\n",
        "* Quantity: The number of units purchased.\n",
        "\n",
        "* Price per Unit: The price of a single unit of the product.\n",
        "\n",
        "* Total Amount: The total cost of the transaction (Quantity * Price per Unit).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sEJ-OvqcaWe"
      },
      "source": [
        "# Preparation: Install Required Libraries\n",
        "In this first step, we will install the necessary libraries to interact with the dataset and use the LangChain tool for querying the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYINSezAMMIa",
        "outputId": "287439b6-1dbf-4d63-9e84-ef98d4b91374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/643.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m471.0/643.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m643.9/643.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# freezing the coding environment to these specific versions\n",
        "!pip install openai==1.72.0 langchain==0.3.23 langchain_openai==0.3.13 langchain_experimental==0.3.4 langchain_community==0.3.21 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x8ILajJce6y"
      },
      "source": [
        "# Preparation: Load API Key and Dataset\n",
        "Now, let's mount Google Drive and load the API key along with the retail sales dataset from the uploaded file into a pandas DataFrame.\n",
        "\n",
        "***If you don't have an OpenAI API key yet, please visit https://platform.openai.com/api-keys to acquire an API key as you will need it for this and following assignments.***\n",
        "\n",
        "Make sure you change the `pth` object to reflect where you upload your assignment notebooks, API key, and dataset file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhHfmRxxMfzG",
        "outputId": "3174e50a-7aae-4663-d4af-9a8c59c7c332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   Store  Dept        Date  Weekly_Sales  IsHoliday Type    Size  Temperature  \\\n",
            "0      1     1  05/02/2010      24924.50      False    A  151315        42.31   \n",
            "1      1     1  12/02/2010      46039.49       True    A  151315        38.51   \n",
            "2      1     1  19/02/2010      41595.55      False    A  151315        39.93   \n",
            "3      1     1  26/02/2010      19403.54      False    A  151315        46.63   \n",
            "4      1     1  05/03/2010      21827.90      False    A  151315        46.50   \n",
            "5      1     1  12/03/2010      21043.39      False    A  151315        57.79   \n",
            "6      1     1  19/03/2010      22136.64      False    A  151315        54.58   \n",
            "7      1     1  26/03/2010      26229.21      False    A  151315        51.45   \n",
            "8      1     1  02/04/2010      57258.43      False    A  151315        62.27   \n",
            "9      1     1  09/04/2010      42960.91      False    A  151315        65.86   \n",
            "\n",
            "   Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5  \\\n",
            "0       2.572        NaN        NaN        NaN        NaN        NaN   \n",
            "1       2.548        NaN        NaN        NaN        NaN        NaN   \n",
            "2       2.514        NaN        NaN        NaN        NaN        NaN   \n",
            "3       2.561        NaN        NaN        NaN        NaN        NaN   \n",
            "4       2.625        NaN        NaN        NaN        NaN        NaN   \n",
            "5       2.667        NaN        NaN        NaN        NaN        NaN   \n",
            "6       2.720        NaN        NaN        NaN        NaN        NaN   \n",
            "7       2.732        NaN        NaN        NaN        NaN        NaN   \n",
            "8       2.719        NaN        NaN        NaN        NaN        NaN   \n",
            "9       2.770        NaN        NaN        NaN        NaN        NaN   \n",
            "\n",
            "          CPI  Unemployment  IsHoliday_f  \n",
            "0  211.096358         8.106        False  \n",
            "1  211.242170         8.106         True  \n",
            "2  211.289143         8.106        False  \n",
            "3  211.319643         8.106        False  \n",
            "4  211.350143         8.106        False  \n",
            "5  211.380643         8.106        False  \n",
            "6  211.215635         8.106        False  \n",
            "7  211.018042         8.106        False  \n",
            "8  210.820450         7.808        False  \n",
            "9  210.622857         7.808        False  \n",
            "Using key: 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\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_experimental.agents import create_pandas_dataframe_agent\n",
        "import openai\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Mount Google Drive if using Colab\n",
        "# try:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "pth = '/content/drive/MyDrive/Colab_Notebooks'\n",
        "# except ModuleNotFoundError:\n",
        "    # # Not in Colab, fallback for local run\n",
        "    # pth = r\"D:\\Downloads\\columbia data\\Colab_Notebooks\"\n",
        "\n",
        "# Change working directory\n",
        "os.chdir(pth)\n",
        "\n",
        "# Read the key from file and set environment variable\n",
        "with open(\"api/openai.txt\", \"r\") as f:\n",
        "    key = f.read().strip()\n",
        "os.environ[\"OPENAI_API_KEY\"] = key   # ✅ Correct env var name\n",
        "\n",
        "# Path to the dataset (since you're already inside Colab_Notebook)\n",
        "file_path = \"reconstructed_merged_full.csv\"\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Show preview\n",
        "print(df.head(10))\n",
        "\n",
        "print(\"Using key:\", os.environ[\"OPENAI_API_KEY\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQkdDioAdBZC"
      },
      "source": [
        "# Preparation: Create LangChain Agent\n",
        "\n",
        "We will now create a LangChain agent that allows us to interact with the dataset through natural language queries. This agent will analyze the data and respond with insights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rrdTqxjHWWsZ"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "# set up the pandas dataframe agent\n",
        "agent_executor = create_pandas_dataframe_agent(llm,\n",
        "                                      df,\n",
        "                                      number_of_head_rows=429761, # Set this to the total number of rows in df\n",
        "                                      agent_type=\"openai-tools\",\n",
        "                                      allow_dangerous_code = True,\n",
        "                                      verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBZ68f2feL54"
      },
      "source": [
        "# Now we got all the setup taken care of, let's get start with our tasks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgKLlDzPVRPi"
      },
      "source": [
        "# Prompt Engineering Tip\n",
        "After some experimentation, I found that it works best when you attach\n",
        "\n",
        "***The dataset is available at data/retail_sales_dataset.csv, and I’d like you to use all 1000 rows. Thank you!***\n",
        "\n",
        "at the end of your prompt. Keep this in mind when you perform the below tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur9Hdajbd6Yk"
      },
      "source": [
        "# Step 1: Data Exploration\n",
        "\n",
        "Use the Langchain agent with pandas to explore and clean the dataset. Identify any missing values, outliers, or trends.\n",
        "\n",
        "## Sample prompt\n",
        "***Can you help me explore and clean the dataset. Identify any missing values, outliers, or trends? The data you need is in data/retail_sales_dataset.csv. The dataset is available at data/retail_sales_dataset.csv, and I’d like you to use all 1000 rows. Thank you!***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "mLWMKJ3Kg-jt",
        "outputId": "044020b7-dbb2-4f77-ed0f-dcd75c233965"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent_executor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1928269442.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mIdentify\u001b[0m \u001b[0many\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutliers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'agent_executor' is not defined"
          ]
        }
      ],
      "source": [
        "# Change this prompt and observe how the response changes\n",
        "prompt = \"\"\"\n",
        "Can you help me explore and clean the dataset.\n",
        "Identify any missing values, outliers, or trends.\n",
        "\"\"\"\n",
        "response = agent_executor.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORsHpRBUKP4s",
        "outputId": "76cfb20e-6bbd-4c70-880d-d553af89c685"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVcjvB_7hpzu"
      },
      "source": [
        "## Let's take a look at the return object\n",
        "The chain consists of both the input we provided to the LangChain agent and the agent’s output after executing the code on our behalf. The agent runs the code, processes the results, and returns the relevant insights, such as the summary statistics, missing values, and trends found in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtbiAH-6hod_"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y39O-PPKWR8f"
      },
      "source": [
        "## Print it out for better readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPtTNwYXhHaD"
      },
      "outputs": [],
      "source": [
        "print(response['output'])\n",
        "# This is markup format, and you can copy this to a text cell for better readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_SUDRK5Vw5K"
      },
      "source": [
        "# Step 2: Sales Analysis\n",
        "\n",
        "Explore when order hikes or drops occur (e.g., specific months, days of the week, or during sales events).\n",
        "\n",
        "## Sample prompt\n",
        "***Could you assist me in conducting a deeper sales analysis on the dataset? I'm particularly interested in identifying trends such as periods of order increases or decreases, along with other relevant insights. Please use visualizations to present the findings clearly. The data you need is in data/retail_sales_dataset.csv. Use all 1000 rows of it. Thank you!***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIg-THSyWh3u"
      },
      "outputs": [],
      "source": [
        "# Change this prompt and observe how the response changes\n",
        "prompt = \"\"\"\n",
        "Could you assist me in conducting a deeper sales analysis on the dataset?\n",
        "I'm particularly interested in identifying trends such as periods of order increases or decreases, along with other relevant insights.\n",
        "Please use visualizations to present the findings clearly.\n",
        "The dataset is available at data/retail_sales_dataset.csv, and I’d like you to use all 1000 rows. Thank you!\n",
        "\"\"\"\n",
        "response = agent_executor.invoke(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jgnh189XPSL"
      },
      "source": [
        "## Review the agent's response and what's your comment?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcLONfrzXI5H"
      },
      "outputs": [],
      "source": [
        "print(response['output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j7N8Ti5fn4-"
      },
      "source": [
        "# Step 3: Customer Insights with Machine Learning\n",
        "Identify different customer groups and which product categories they tend to favor the most. This might involve clustering based on spending patterns, demographics, or product categories.\n",
        "\n",
        "## Sample Prompt\n",
        "***Could you help me identify distinct customer groups based on their spending patterns, demographics, and product preferences? I'd also like recommendations on sales strategies for each group. Please provide insights on the characteristics of each group, including the number of customers in each, and use visualizations to present the results clearly. The dataset is available at data/retail_sales_dataset.csv, and I’d like you to use all 1000 rows. Thank you!***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVKD0FYvf0vJ"
      },
      "outputs": [],
      "source": [
        "# Change this prompt and observe how the response changes\n",
        "prompt = \"\"\"\n",
        "Could you help me identify distinct customer groups based on their spending patterns, demographics, and product preferences?\n",
        "I'd also like recommendations on sales strategies for each group.\n",
        "Please provide insights on the characteristics of each group, including the number of customers in each.\n",
        "Use visualizations to present the results clearly.\n",
        "The dataset is available at data/retail_sales_dataset.csv, and I’d like you to use all 1000 rows. Thank you!\n",
        "\"\"\"\n",
        "response = agent_executor.invoke(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdB4yQQ8YrLs"
      },
      "source": [
        "## Let's review the agent's response closely\n",
        "\n",
        "* Does the clustering make sense to you?\n",
        "* How about the sales strategies for each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zqCWJeLixyq"
      },
      "outputs": [],
      "source": [
        "print(response['output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ47SaIcjNqh"
      },
      "source": [
        "# Step 4: Hypothesis Testing\n",
        "Based on everything we learn about our customers in Task 1, 2, and 3, we might have some hypothesis about our customers and we will be testing a few hypothesis to see if they are statistically sound.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbUNSQsxeYsm"
      },
      "source": [
        "## Hypothesis 1: Customer age has no effect on quantity of purchase.\n",
        "\n",
        "## Sample prompt:\n",
        "***Please help me test the following hypothesis:\n",
        "Null Hypothesis: Age has no effect on quantity of purchase.\n",
        "Alternate hypothesis: Age has an effect on quantity of purchase.\n",
        "The dataset is available at data/retail_sales_dataset.csv, and I’d like you to use all 1000 rows.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBuUkEp8ji5m"
      },
      "outputs": [],
      "source": [
        "# Change this prompt and observe how the response changes\n",
        "prompt = \"\"\"\n",
        "Please help me test the following hypothesis:\n",
        "Null Hypothesis: Age has no effect on quantity of purchase.\n",
        "Alternate hypothesis: Age has an effect on quantity of purchase.\n",
        "The dataset is available at data/retail_sales_dataset.csv, and I’d like you to use all 1000 rows.\n",
        "\"\"\"\n",
        "response = agent_executor.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv4Q1wtAdqba"
      },
      "outputs": [],
      "source": [
        "print(response['output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBb3lLWees4P"
      },
      "source": [
        "## Hypothesis 2: Sales time (such as month and weekdays) has no effect on the total amount of purchase.\n",
        "\n",
        "## Sample prompt:\n",
        "***Please help me test the following hypothesis:\n",
        "Null Hypothesis: Sales time (such as month and weekdays) has no effect on the total amount of purchase.\n",
        "Alternative Hypothesis: Sales time (such as month and weekdays) does have an effect on the total amount of purchase.\n",
        "The dataset is available at data/retail_sales_dataset.csv, and I’d like you to use all 1000 rows.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgwLOeN4dx8Z"
      },
      "outputs": [],
      "source": [
        "# Change this prompt and observe how the response changes\n",
        "prompt = \"\"\"\n",
        "Please help me test the following hypothesis:\n",
        "Null Hypothesis: Sales time (such as month and weekdays) has no effect on the total amount of purchase.\n",
        "Alternative Hypothesis: Sales time (such as month and weekdays) does have an effect on the total amount of purchase.\n",
        "The dataset is available at data/retail_sales_dataset.csv, and I’d like you to use all 1000 rows.\n",
        "\"\"\"\n",
        "response = agent_executor.invoke(prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXMvQHQffHFB"
      },
      "outputs": [],
      "source": [
        "print(response['output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84X_JUkHfNPl"
      },
      "source": [
        "## What other hypothesis do you have? Test it out here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uCFcH16fUdi"
      },
      "source": [
        "# Summary\n",
        "\n",
        "In this notebook, we leveraged the power of LangChain and LLMs to perform advanced retail data analysis tasks. By interacting with retail sales dataset through a LangChain pandas DataFrame agent, we were able to:\n",
        "\n",
        "* **Explore and Clean the Data:** We identified missing values, outliers, and initial trends in the dataset, ensuring data quality for further analysis.\n",
        "* **Analyze Sales Trends:** We uncovered insights into sales patterns, including periods of order increases and decreases, providing valuable information for strategic decision-making.\n",
        "* **Gain Customer Insights:** We identified distinct customer groups based on spending habits, demographics, and product preferences. These insights helped in understanding customer behavior and tailoring sales strategies.\n",
        "* **Test Hypotheses:** We tested statistically sound hypotheses about customer behavior, such as the relationship between age and purchase quantity and the impact of sales time on purchase amount.\n",
        "\n",
        "This approach demonstrates the potential of LLMs in simplifying complex data analysis tasks, enabling analysts to focus on extracting valuable insights and making data-driven decisions. By using natural language to interact with the data, we streamlined the analysis process and gained a deeper understanding of the Columbia Plus Mart retail data. This approach can be applied to various datasets and industries, unlocking new possibilities for data exploration and analysis."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}